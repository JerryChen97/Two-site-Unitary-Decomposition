/root/xanadu/pennylane/pennylane/__init__.py:184: PennyLaneDeprecationWarning: QubitDevice will no longer be accessible top level. Please access the class as pennylane.devices.QubitDevice
  warn(
Initial cost: 0.6638400851486769
Initial measurements: [array(0.67685942), array(0.70952836), array(-0.19223388)]
Gradient: [ 0.01840914 -1.73246924 -0.32800035  0.04225317  0.29189525 -0.32800035
  0.01052006 -1.74684272 -0.49614473]
Initial gradient: [ 0.01840914 -1.73246924 -0.32800035  0.04225317  0.29189525 -0.32800035
  0.01052006 -1.74684272 -0.49614473]

Starting optimization:
Gradient: [-0.19597754 -0.65355597  0.02537157 -0.00711238  0.03866419  0.02537157
 -0.07540416 -0.66655371 -0.1181162 ]
Iter:    20 | Cost: 0.1110354 | Grad norm: 0.9655603
Measurements: [array(0.31942772), array(0.94585879), array(-0.05602485)]
Gradient: [-0.00998927  0.00537012  0.01657767  0.00117455 -0.00183367  0.01657767
 -0.00563227 -0.00233295  0.01795723]
Iter:    40 | Cost: 0.0001217 | Grad norm: 0.0322898
Measurements: [array(0.00120674), array(0.99550727), array(0.00893845)]
Gradient: [ 0.02513377  0.07550598  0.00284391  0.00683716 -0.00633602  0.00284391
  0.01159003  0.07199463  0.02948632]
Iter:    60 | Cost: 0.0016496 | Grad norm: 0.1123518
Measurements: [array(-0.03736086), array(0.99562283), array(0.01467891)]
Gradient: [-0.0056954  -0.01684501 -0.00017011 -0.00076502 -0.00024898 -0.00017011
 -0.00256597 -0.01643089 -0.00567377]
Iter:    80 | Cost: 0.0000836 | Grad norm: 0.0250129
Measurements: [array(0.0084868), array(0.99866919), array(-0.00283311)]
Gradient: [-0.0016006  -0.00369846  0.0004416  -0.00010421 -0.000213    0.0004416
 -0.00075606 -0.00381795 -0.00079305]
Iter:   100 | Cost: 0.0000065 | Grad norm: 0.0056977
Measurements: [array(0.00197429), array(0.99888708), array(-0.00039608)]
Gradient: [ 0.00085886  0.00337053  0.00044068  0.0002229  -0.00018226  0.00044068
  0.00036143  0.00309968  0.00154548]
Iter:   120 | Cost: 0.0000054 | Grad norm: 0.0049696
Measurements: [array(-0.00160382), array(0.99893875), array(0.00077192)]
Gradient: [-4.35655709e-04 -1.31186351e-03 -2.85391687e-05  2.28009101e-06
 -1.76615552e-04 -2.85391687e-05 -1.95809340e-04 -1.27174892e-03
 -4.62954023e-04]
Iter:   140 | Cost: 0.0000024 | Grad norm: 0.0019528
Measurements: [array(0.00065784), array(0.99903006), array(-0.00023125)]
Gradient: [ 1.45769265e-04  3.52770581e-04 -3.21728335e-05  7.06471152e-05
 -1.52641877e-04 -3.21728335e-05  6.86757875e-05  3.59032699e-04
  8.43011774e-05]
Iter:   160 | Cost: 0.0000017 | Grad norm: 0.0005628
Measurements: [array(-0.00018575), array(0.99908557), array(4.21120803e-05)]
Gradient: [-5.74877677e-05 -1.33833089e-04  1.52706261e-05  4.37353921e-05
 -1.40025543e-04  1.52706261e-05 -2.70288823e-05 -1.38598932e-04
 -3.06869577e-05]
Iter:   180 | Cost: 0.0000015 | Grad norm: 0.0002531
Measurements: [array(7.17097821e-05), array(0.99914916), array(-1.53304351e-05)]
Gradient: [ 1.10619771e-05  2.17561592e-05 -4.87977382e-06  4.51054901e-05
 -1.24392064e-04 -4.87977382e-06  5.45517783e-06  2.25925318e-05
  1.33245213e-06]
Iter:   200 | Cost: 0.0000013 | Grad norm: 0.0001367
Measurements: [array(-1.1690103e-05), array(0.99920877), array(6.65699346e-07)]
Gradient: [-1.16449032e-05 -3.18078036e-05  8.07511613e-07  3.80399789e-05
 -1.11403735e-04  8.07511613e-07 -5.30193439e-06 -3.24601031e-05
 -1.07976613e-05]
Iter:   220 | Cost: 0.0000011 | Grad norm: 0.0001273
Measurements: [array(1.67969573e-05), array(0.99926718), array(-5.39487717e-06)]
Gradient: [-2.23828920e-06 -4.28894200e-06  1.04227937e-06  3.46026957e-05
 -9.86269622e-05  1.04227937e-06 -1.07623445e-06 -5.67869998e-06
 -1.36886353e-06]
Iter:   240 | Cost: 0.0000009 | Grad norm: 0.0001048
Measurements: [array(2.9387201e-06), array(0.99932291), array(-6.83968655e-07)]
Gradient: [-4.27520261e-06 -1.20294792e-05  1.24864639e-07  3.02218850e-05
 -8.73629882e-05  1.24864639e-07 -1.97099092e-06 -1.28655506e-05
 -4.73872284e-06]
Iter:   260 | Cost: 0.0000008 | Grad norm: 0.0000943
Measurements: [array(6.65830387e-06), array(0.99937598), array(-2.36788381e-06)]
Gradient: [-2.58736326e-06 -7.10959345e-06  1.58420987e-07  2.66892024e-05
 -7.69827916e-05  1.58420987e-07 -1.22932473e-06 -8.06373156e-06
 -2.99180086e-06]
Iter:   280 | Cost: 0.0000007 | Grad norm: 0.0000823
Measurements: [array(4.1734582e-06), array(0.99942603), array(-1.49504232e-06)]
Gradient: [-2.07470890e-06 -5.56474438e-06  1.93154502e-07  2.34116302e-05
 -6.76836707e-05  1.93154502e-07 -1.01684569e-06 -6.54789475e-06
 -2.36463841e-06]
Iter:   300 | Cost: 0.0000006 | Grad norm: 0.0000722
Measurements: [array(3.38910262e-06), array(0.999473), array(-1.18169645e-06)]
Gradient: [-1.72242974e-06 -4.54595566e-06  1.96235447e-07  2.04735130e-05
 -5.93718906e-05  1.96235447e-07 -8.70425784e-07 -5.51883073e-06
 -1.94644732e-06]
Iter:   320 | Cost: 0.0000005 | Grad norm: 0.0000633
Measurements: [array(2.85661331e-06), array(0.99951683), array(-9.72753653e-07)]
Gradient: [-1.29518875e-06 -3.34951134e-06  1.80995687e-07  1.78733644e-05
 -5.19687870e-05  1.80995687e-07 -6.84617807e-07 -4.29733005e-06
 -1.49187521e-06]
Iter:   340 | Cost: 0.0000004 | Grad norm: 0.0000553
Measurements: [array(2.22445089e-06), array(0.99955755), array(-7.45607711e-07)]
Gradient: [-9.62840577e-07 -2.43500191e-06  1.61282014e-07  1.55699565e-05
 -4.54118142e-05  1.61282014e-07 -5.38338325e-07 -3.34267969e-06
 -1.14026216e-06]
Iter:   360 | Cost: 0.0000003 | Grad norm: 0.0000482
Measurements: [array(1.73036215e-06), array(0.99959525), array(-5.69900411e-07)]
Gradient: [-7.12004855e-07 -1.74015370e-06  1.48667731e-07  1.35381765e-05
 -3.96260138e-05  1.48667731e-07 -4.26905743e-07 -2.60178544e-06
 -8.60001750e-07]
Iter:   380 | Cost: 0.0000003 | Grad norm: 0.0000420
Measurements: [array(1.34688442e-06), array(0.99963003), array(-4.29841847e-07)]
Gradient: [-5.10764815e-07 -1.19347918e-06  1.33310073e-07  1.17532697e-05
 -3.45364032e-05  1.33310073e-07 -3.35673004e-07 -2.00243366e-06
 -6.37057512e-07]
Iter:   400 | Cost: 0.0000002 | Grad norm: 0.0000366
Measurements: [array(1.03664997e-06), array(0.99966203), array(-3.1842114e-07)]
Gradient: [-3.52033802e-07 -7.66420586e-07  1.19189024e-07  1.01899865e-05
 -3.00713658e-05  1.19189024e-07 -2.62155678e-07 -1.52017865e-06
 -4.58026497e-07]
Iter:   420 | Cost: 0.0000002 | Grad norm: 0.0000318
Measurements: [array(7.87013922e-07), array(0.99969141), array(-2.28942598e-07)]
Gradient: [-2.28421402e-07 -4.38556527e-07  1.05905148e-07  8.82431406e-06
 -2.61633666e-05  1.05905148e-07 -2.03362522e-07 -1.13617798e-06
 -3.16344390e-07]
Iter:   440 | Cost: 0.0000002 | Grad norm: 0.0000276
Measurements: [array(5.88229305e-07), array(0.99971831), array(-1.58127652e-07)]
Gradient: [-1.33408155e-07 -1.91119245e-07  9.34732145e-08  7.63392951e-06
 -2.27496170e-05  9.34732145e-08 -1.56661119e-07 -8.33051998e-07
 -2.05509115e-07]
Iter:   460 | Cost: 0.0000001 | Grad norm: 0.0000240
Measurements: [array(4.31304224e-07), array(0.99974292), array(-1.02728148e-07)]
Gradient: [-6.14148691e-08 -7.90406760e-09  8.19763740e-08  6.59833343e-06
 -1.97725201e-05  8.19763740e-08 -1.19792950e-07 -5.95675756e-07
 -1.19746236e-07]
Iter:   480 | Cost: 0.0000001 | Grad norm: 0.0000209
Measurements: [array(3.0841233e-07), array(0.99976538), array(-5.98590741e-08)]
Gradient: [-7.78155823e-09  1.24553021e-07  7.14500343e-08  5.69889385e-06
 -1.71798025e-05  7.14500343e-08 -9.08632739e-08 -4.11353331e-07
 -5.42033720e-08]
Iter:   500 | Cost: 0.0000001 | Grad norm: 0.0000181
Measurements: [array(2.12983527e-07), array(0.99978587), array(-2.70958839e-08)]

Final tests:
Final measurements: [array(2.08831403e-07), array(0.99978684), array(-2.5684468e-08)]
Final cost: 9.087112005887917e-08
Final weights: [ 0.47696883  0.34480342  0.20037644 -0.03289926 -0.0322279   0.00477485
  0.38180756  0.28966971  0.26349312]

Cost progress:
Step 20: 0.1110354
Step 40: 0.0001217
Step 60: 0.0016496
Step 80: 0.0000836
Step 100: 0.0000065
Step 120: 0.0000054
Step 140: 0.0000024
Step 160: 0.0000017
Step 180: 0.0000015
Step 200: 0.0000013
Step 220: 0.0000011
Step 240: 0.0000009
Step 260: 0.0000008
Step 280: 0.0000007
Step 300: 0.0000006
Step 320: 0.0000005
Step 340: 0.0000004
Step 360: 0.0000003
Step 380: 0.0000003
Step 400: 0.0000002
Step 420: 0.0000002
Step 440: 0.0000002
Step 460: 0.0000001
Step 480: 0.0000001
Step 500: 0.0000001
